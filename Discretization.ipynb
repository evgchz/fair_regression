{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairRegressionDiscret():\n",
    "    def __init__(self, base_method, beta=1, L=20, num_iter=1000, M=10, weights=[.5, .5]):\n",
    "        self.base_method = base_method\n",
    "        self.beta = beta\n",
    "        self.L = L\n",
    "        self.num_iter = num_iter\n",
    "        self.M = M\n",
    "        self.weights = weights\n",
    "    def fit(self, X_unlab):\n",
    "        coef = np.zeros(2 * self.L + 1)\n",
    "        moment = np.zeros(2 * self.L + 1)\n",
    "        y_pred0 = self.base_method.predict(X_unlab[X_unlab[:,-1] == -1])\n",
    "        y_pred1 = self.base_method.predict(X_unlab[X_unlab[:,-1] == 1])\n",
    "        discr = np.arange(-self.L, self.L + 1) * self.M / self.L\n",
    "        z0 = self.weights[0] * np.square(y_pred0[:, np.newaxis] - discr)\n",
    "        z1 = self.weights[1] * np.square(y_pred1[:, np.newaxis] - discr)\n",
    "        tau = 0\n",
    "        for t in range(self.num_iter):\n",
    "            tmp = (1 + np.sqrt(1 + 4 * tau ** 2)) / 2\n",
    "            gamma = (1 - tau) / tmp\n",
    "            tau = tmp\n",
    "            coef_prev = coef\n",
    "            coef = moment - (self.beta / 2) *  (np.mean(softmax((moment - z1) / self.beta, axis=1), axis=0) - np.mean(softmax((-moment - z0) / self.beta, axis=1), axis=0))\n",
    "            moment = (1 - gamma) * coef + gamma * coef_prev\n",
    "        self.coef_ = coef\n",
    "    def predict(self, X):\n",
    "        n_samples, _ = X.shape\n",
    "        s = np.zeros(n_samples)\n",
    "        discr = np.arange(-self.L, self.L + 1) * self.M / self.L\n",
    "        s[X[:,-1] == -1] = -1\n",
    "        s[X[:,-1] == 1] = 1\n",
    "        y_pred = self.base_method.predict(X)\n",
    "        z =  np.square(y_pred[:, np.newaxis] - discr)\n",
    "        z[X[:,-1] == -1, :] *= self.weights[0]\n",
    "        z[X[:,-1] == 1, :] *= self.weights[1]\n",
    "        return (np.argmin(-s[:,np.newaxis] * self.coef_ + z, axis=1) - self.L) * self.M / self.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 500\n",
    "thr = 0\n",
    "\n",
    "X_train = np.random.normal(0, 1, (N_train, 3))\n",
    "sens = np.zeros(N_train)\n",
    "sens[X_train[:, 0] < thr] = -1\n",
    "sens[X_train[:, 0] >= thr] = 1\n",
    "sens = sens[:, np.newaxis]\n",
    "X_train = np.append(X_train, sens, axis=1)\n",
    "y_train = np.dot(X_train, np.ones(4) * 3) + np.random.normal(0, 1, N_train)\n",
    "\n",
    "\n",
    "N_unlab = 1000\n",
    "X_unlab = np.random.normal(0, 1, (N_unlab, 3))\n",
    "sens = np.zeros(N_unlab)\n",
    "sens[X_unlab[:, 0] < thr] = -1\n",
    "sens[X_unlab[:, 0] >= thr] = 1\n",
    "sens = sens[:, np.newaxis]\n",
    "X_unlab = np.append(X_unlab, sens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LinearRegression(fit_intercept=False)\n",
    "ls.fit(X_train, y_train)\n",
    "weights = [.5, .5]\n",
    "fair_ls =  FairRegressionDiscret(ls, M=20, weights=weights, L=100, num_iter=20000, beta=1)\n",
    "fair_ls.fit(X_unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fair_ls.predict(X_unlab)\n",
    "# print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred0 = y_pred[X_unlab[:, -1] == -1]\n",
    "y_pred1 = y_pred[X_unlab[:, -1] == 1]\n",
    "\n",
    "plt.figure('1')\n",
    "plt.hist(y_pred0, label='s=0', density=True, stacked=True)\n",
    "plt.hist(y_pred1, label='s=1', alpha=0.3, density=True, stacked=True)\n",
    "plt.title('Performance on unlabeled (in-sample) data with fairness adjustment')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ls.predict(X_unlab)\n",
    "y_pred0 = y_pred[X_unlab[:, -1] == -1]\n",
    "y_pred1 = y_pred[X_unlab[:, -1] == 1]\n",
    "\n",
    "plt.figure('1')\n",
    "plt.hist(y_pred0, label='s=0', density=True, stacked=True)\n",
    "plt.hist(y_pred1, label='s=1', alpha=0.3, density=True, stacked=True)\n",
    "plt.title('Performance on unlabeled (in-sample) data with fairness adjustment')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
